{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r0brt/Daan_Semesterarbeit/blob/master/HaemmerliRobert_Daan_QLearning_FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEblvbRIHCBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3TiwnMWHDE9",
        "colab_type": "text"
      },
      "source": [
        "# Frozen Lake\n",
        "![alt text](http://deeplizard.com/images/frozen%20lake%20winter.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iwddHx1HZ71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eisSd85XHfyE",
        "colab_type": "text"
      },
      "source": [
        "### Setting up Frozen Lake\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-DfGbl9Hooo",
        "colab_type": "text"
      },
      "source": [
        "#### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NL947AEHzUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5lY4y65H0Ri",
        "colab_type": "text"
      },
      "source": [
        "#### Creating the environment\n",
        "\n",
        "Next, to create our environment, we just call gym.make() and pass a string of the name of the environment we want to set up. We'll be using the environment FrozenLake-v0. All the environments with their corresponding names you can use here are available on Gym’s website https://gym.openai.com/envs/#classic_control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwyW2PS3H9yK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xwb1AcQIVX5",
        "colab_type": "text"
      },
      "source": [
        "#### Creating the Q-table\n",
        "\n",
        "We’re now going to construct our Q-table, and initialize all the Q-values to zero for each state-action pair.\n",
        "\n",
        "Remember, the number of rows in the table is equivalent to the size of the state space in the environment, and the number of columns is equivalent to the size of the action space. We can get this information using using env.observation_space.n and env.action_space.n, as shown below. We can then use this information to build the Q-table and fill it with zeros. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpeyDVGUIOVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "\n",
        "q_table = np.zeros((state_space_size, action_space_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcjiQAIxIZzP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "0ea44a82-71f3-4f57-93ff-737de589cf15"
      },
      "source": [
        "print(q_table)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WuwdPSKI_3x",
        "colab_type": "text"
      },
      "source": [
        "#### Initializing Q-learning parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0UaGhkPJOBr",
        "colab_type": "text"
      },
      "source": [
        "First, with **num_episodes**, we define the total number of episodes we want the agent to play during training. Then, with **max_steps_per_episode**, we define a maximum number of steps that our agent is allowed to take within a single episode. So, if by the one-hundredth step, the agent hasn’t reached the frisbee or fallen through a hole, then the episode will terminate with the agent receiving zero points.\n",
        "\n",
        "Next, we set our **learning_rate**, which was mathematically shown using the symbol α\n",
        "in the previous post. Then, we also set our **discount_rate**, as well, which was represented with the symbol γ previously.\n",
        "\n",
        "Now, the last four parameters are all for related to the exploration-exploitation trade-off we talked about last time in regards to the epsilon-greedy po licy. We’re initializing our **exploration_rate** to 1 and setting the **max_exploration_rate** to 1 and a **min_exploration_rate** to 0.01. The max and min are just bounds to how large or small our exploration rate can be.\n",
        "Remember, the exploration rate was represented with the symbol ϵwhen we discussed it previously.\n",
        "\n",
        "Lastly, we set the **exploration_decay_rate** to 0.01 to determine the rate at which the exploration_rate will decay. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGloGWdLIhaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_episodes = 10000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "learning_rate = 0.1\n",
        "discount_rate = 0.99\n",
        "\n",
        "exploration_rate = 1\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPaNjE4KJ7TN",
        "colab_type": "text"
      },
      "source": [
        "#### The Q-learning algorithm training loop "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD61fjNIN_B3",
        "colab_type": "text"
      },
      "source": [
        "`Update Q-table for Q(s,a)`:  \n",
        "The new Q-value for this state-action pair is a weighted sum of our old value and the “learned value.” So we have our new Q-value equal to the old Q-value times one minus the learning rate plus the learning rate times the “learned value,” which is the reward we just received from our last action plus the discounted estimate of the optimal future Q-value for the next state action pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT6uVgcGJESR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list to hold all of the rewards\n",
        "rewards_all_episodes = []\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # initialize new episode params\n",
        "    state = env.reset()\n",
        "    done = False                 # keeps track of whether or not our episode is finished\n",
        "    rewards_current_episode = 0  # keep track of the rewards\n",
        "    \n",
        "    for step in range(max_steps_per_episode): \n",
        "        # Exploration-exploitation trade-off\n",
        "        exploration_rate_threshold = random.uniform(0, 1)\n",
        "        if exploration_rate_threshold > exploration_rate:  # exploit the environment and choose the action that has the highest Q-value\n",
        "            action = np.argmax(q_table[state,:]) \n",
        "        else:                                              # agent will explore the environment, and sample an action randomly\n",
        "            action = env.action_space.sample()\n",
        "            \n",
        "        # Take new action\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        # Update Q-table for Q(s,a)\n",
        "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :])\n",
        "        \n",
        "        # Set new state\n",
        "        # Add new reward        \n",
        "\n",
        "    # Exploration rate decay   \n",
        "    # Add current episode reward to total rewards list"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}